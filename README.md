# Recommender_systems-UserKNN-Lenskit-Surprise-Caserec
Studien Arbeit"/"Student Project" (research project) under the supervision of Prof. Dr.-Ing. Joeran Beel, head of the Intelligent Systems Group (ISG), University of Siegen, on the research topic "Consistency of the algorithm implementations of different Recommender-systems frameworks"


STUDIENARBEIT Topic - The consistency of the Algorithm - Implementation of different Recommender systems framework. Here is the complete explaination for the Pipeline for comparision of Lenskit, Surprise and Caserec frameworks implementing the User-based K-nearest neighbors algorithm There are 4 stages in this pipeline

Loading the data required
Splitting the dataset
creating the models using userKNN algorithm with Lenskit, Surprise and Caserec frameworks and train (fit()) and test them (get predictions) using the splitted data.
Evaluation of the performance of the three frameworks using the metrics(RMSE & MAE).
1.### lOADING THE DATA 1. First download the movielens 100k dataset and use the u.data - rating file only, as we are dealing with user, item, rating. 2. Then copy paste the u.data file path and use it for converting the text file to pandas DataFrame using pandas.read_csv().

2.### SPLITTING THE DATASET 1. Here the loaded dataset is splitted into 5 folds using crossfold.partition_users() method from Lenskit library 2. Each fold consists of a trainset and testset 3. As partition_users() method is an iterator of train-test pairs, here tain_set and test_set list objects are created and using for loop all the trainsets and testsets are appended into those respective list objects. 4. And the partitionMethod - SampleFrac() used in partition_users(), randomly select a fraction of test rows per user/item. 5. Now all the 5 trainsets and testsets are ready for use

3.### USERKNN ALGORITHM IMPLEMENTATION USING LENSKIT,SURPRISE AND CASEREC FRAMEWORKS 1. First using Lenskit library - created a lenskit_model object list and by using lenskit's userKNN algo, created 5 models and appended it to the list object. Because, when we train a single model using all the 5 trainsets at once, the data of one fold trainset may be preset in the testset of other folds, so its better to build the separate models using same userKNN algo for each fold and train and test the model using that respective fold's train and testset. 2. Then defined a function called get_ratings_and_predictions(), where we can pass all the 5 models and 5 train and testsets respectively using a for loop while calling the function defined. This function return the ratings and predictions of all the 5 folds in the form of lists. 3. all the ratings and prediction lists got by calling the function defined above are appended into one lists respectively, So later we can use them for evaluation. 4. Now, a function called Metric_Eval() is defined, where the parameters are the ratings and predictions we got in the above step, this function returns both RMSE & MAE values 5. Now using for loop all the 5 folds ratings and predictions are passed to Metric_Eval() function and appended all the 5 folds RMSE & MAE in to lists separatly. 6. Now using numpy.mean(), found the mean of 5 folds RMSE & MAE.

NOW using Surprise Library: 1. Same as we done in Lenskit, created 5 separate models using surprise UserKNN algorithm. 2. Here the requirments of trainset for surprise is different. So, trainsets are built by build_full_trainset() method from the trainsets which we splitted above. 3. as the predict() method in surprise expects the userid and itemid for predicting the ratings, we need to supply them one by one using iterator(for loop), for that all the userids and itemids from every fold are collected into lists separatly. 4. Now, a function named get_predictions() is defined where the parameters are the models we created above, trainsets, userids and itemids collected in lists and it returns predictions list. 5. by using for loop, collected predictions of all the folds into one list, by calling the get_predictions() function. 6. And same as in lenskit using Metric_Eval function, collected RMSE & MAE values of all the folds and calculated the mean of them.

NOW using Caserec Library: 1. In caserec the data format is different from other libraries, the data should be in the text file format, so all the train and test datasets of every fold should be coverted to text format using to_csv() method. NOTE: The datasets which we convert them to text files will be saved in the folder, where our IDE files are actually present in the pc. 2. Now, all the saved train and testsets text file's paths should be pasted as shown in the pipeline,, later we supply those paths to the model to get predictions. 3. Now, we should create 5 prediction_output textfiles manually in one of the folders in our pc and copy paste the 5 files paths in IDE as shown in the pipeline. 4. as we did in lenskit and surprise, create 5 separate models for 5 folds usign caserec's userKNN algorithm. 5. defined a function called get_predictions_caserec(), where it just take model as parameter, where the model expects teh train, test and predictions output file paths we pasted in above steps. And it returns predictions of all folds,as we give 5 models as parameters using for loop. Then the predictions should be converted to pandas DataFrame, as we need them for model evaluation later

NOTE: The predictions we get, will be sorted by user and then by items, but in testset the data is sorted by users only, so the items will be not as in the predictions, therefore we have to sort the testsets data by users and then by items,now both the user and item pairs will be same in prediction dataframe and testsets dataframe 6.Now the testsets ratings and predictions from predicion df of all the folds are to be appended to a single lists(consisting 5 lists), and using Metric_Eval() function we have to find the RMSE & MAE values for 5 folds, then calculate the mean of RMSE & MAE of 5 folds.

4.### Now, after finding RMSE and MAE values of three frameworks, made a dataframe consisting the metrics as columns and frameworks as rows, by using the dataframe and using pandas and matplotlib libraries, a bar chart is plotted to get a visualization for comparing the metrics of the three frameworks.
